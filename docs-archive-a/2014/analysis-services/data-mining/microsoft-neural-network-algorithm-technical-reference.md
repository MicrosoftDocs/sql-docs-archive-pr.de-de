---
title: Technische Referenz für den Microsoft Neural Network-Algorithmus | Microsoft-Dokumentation
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: de-DE
ms.lasthandoff: 08/04/2020
ms.locfileid: "87608100"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="66af5-102">Technische Referenz für den Microsoft Neural Network-Algorithmus</span><span class="sxs-lookup"><span data-stu-id="66af5-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="66af5-103">Der [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network-Algorithmus verwendet ein *mehrschichtiges Perzeptronnetzwerk* , das auch als *Netzwerk von Deltaregeln mit Rückpropagierung*bezeichnet wird. Es besteht aus bis zu drei Ebenen aus Neuronen oder *Perzeptronen*.</span><span class="sxs-lookup"><span data-stu-id="66af5-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="66af5-104">Zu diesen Ebenen gehört eine Eingabeebene, eine optionale verborgene Ebene und eine Ausgabeebene.</span><span class="sxs-lookup"><span data-stu-id="66af5-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="66af5-105">Eine detaillierte Erläuterung von mehrschichtigen Perzeptronnetzwerken geht über den Rahmen dieser Dokumentation hinaus.</span><span class="sxs-lookup"><span data-stu-id="66af5-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="66af5-106">In diesem Thema wird die grundlegende Implementierung des Algorithmus, einschließlich der Methode zum Normalisieren von Eingabe- und Ausgabewerten, erläutert sowie Funktionsauswahlverfahren zur Reduzierung der Attributkardinalität beschrieben.</span><span class="sxs-lookup"><span data-stu-id="66af5-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="66af5-107">Das Thema enthält eine Beschreibung der Parameter und anderer Einstellungen, mit denen das Verhalten des Algorithmus angepasst wird. Ferner werden Links zu weiteren Informationen über das Abfragen von Modellen zur Verfügung gestellt.</span><span class="sxs-lookup"><span data-stu-id="66af5-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="66af5-108">Implementierung des Microsoft Neural Network-Algorithmus</span><span class="sxs-lookup"><span data-stu-id="66af5-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="66af5-109">In einem mehrschichtigen Perzeptronnetzwerk empfängt jedes Neuron mindestens eine Eingabe bzw. erstellt mindestens eine Ausgabe.</span><span class="sxs-lookup"><span data-stu-id="66af5-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="66af5-110">Bei jeder Ausgabe handelt es sich um eine einfache nichtlineare Funktion der Summe der Eingaben im Neuron.</span><span class="sxs-lookup"><span data-stu-id="66af5-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="66af5-111">Eingaben werden nur von Knoten der Eingabeebene an die Knoten der verborgenen Ebene und von der verborgenen Ebene an die Ausgabeebene weitergegeben; zwischen den Neuronen innerhalb einer Ebene sind keine Verbindungen vorhanden.</span><span class="sxs-lookup"><span data-stu-id="66af5-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="66af5-112">Wenn keine verborgene Ebene eingeschlossen ist, wie in einem logistischen Regressionsmodell, werden Eingaben direkt von den Knoten der Eingabeebene an die der Ausgabeebene weitergegeben.</span><span class="sxs-lookup"><span data-stu-id="66af5-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="66af5-113">In einem Netzwerk, das mit dem [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network-Algorithmus erstellt wurde, gibt es drei Neuronentypen:</span><span class="sxs-lookup"><span data-stu-id="66af5-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="66af5-114">Eingabeneuronen stellen Eingabeattributwerte des Data Mining-Modells dar.</span><span class="sxs-lookup"><span data-stu-id="66af5-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="66af5-115">Bei diskreten Eingabeattributen stellt ein Eingabeneuron in der Regel einen einzelnen Status des Eingabeattributs dar.</span><span class="sxs-lookup"><span data-stu-id="66af5-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="66af5-116">Hierzu gehören auch fehlende Werte, wenn die Trainingsdaten NULL-Werte für dieses Attribut enthalten.</span><span class="sxs-lookup"><span data-stu-id="66af5-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="66af5-117">Ein diskretes Eingabeattribut mit mehr als zwei Status generiert ein Eingabeneuron für jeden Status sowie ein Eingabeneuron für einen fehlenden Status, wenn NULL-Werte in den Trainingsdaten vorhanden sind.</span><span class="sxs-lookup"><span data-stu-id="66af5-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="66af5-118">Ein kontinuierliches Eingabeattribut generiert zwei Eingabeneuronen: ein Neuron für einen fehlenden Status und ein Neuron für den Wert des kontinuierlichen Attributs selbst.</span><span class="sxs-lookup"><span data-stu-id="66af5-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="66af5-119">Mit Eingabeneuronen werden einem oder mehreren verborgenen Neuronen Eingaben zur Verfügung gestellt.</span><span class="sxs-lookup"><span data-stu-id="66af5-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="66af5-120">Verborgene Neuronen empfangen die von Eingabeneuronen bereitgestellten Eingaben und stellen Ausgabeneuronen die empfangenen Ausgaben zur Verfügung.</span><span class="sxs-lookup"><span data-stu-id="66af5-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="66af5-121">Ausgabeneuronen stellen Ausgabeattributwerte des Data Mining-Modells dar.</span><span class="sxs-lookup"><span data-stu-id="66af5-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="66af5-122">Bei diskreten Eingabeattributen stellt ein Ausgabeneuron in der Regel einen einzelnen Status eines vorhersagbaren Attributs dar. Hierzu gehören auch fehlende Werte.</span><span class="sxs-lookup"><span data-stu-id="66af5-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="66af5-123">Ein binäres vorhersagbares Attribut erstellt z. B. einen Ausgabeknoten, der einen fehlenden oder vorhandenen Status beschreibt, und gibt somit an, ob für dieses Attribut ein Wert vorhanden ist.</span><span class="sxs-lookup"><span data-stu-id="66af5-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="66af5-124">Eine boolesche Spalte, die als vorhersagbares Attribut verwendet wird, generiert drei Ausgabeneuronen: ein Neuron für einen TRUE-Wert, ein Neuron für einen FALSE-Wert und ein Neuron für einen fehlenden oder vorhandenen Status.</span><span class="sxs-lookup"><span data-stu-id="66af5-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="66af5-125">Ein diskretes vorhersagbares Attribut, das über mehr als zwei Status verfügt, generiert ein Ausgabeneuron für jeden Status sowie ein Ausgabeneuron für einen fehlenden oder vorhandenen Status.</span><span class="sxs-lookup"><span data-stu-id="66af5-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="66af5-126">Kontinuierliche vorhersagbare Spalten generieren zwei Ausgabeneuronen: ein Neuron für einen fehlenden oder vorhandenen Status und ein Neuron für den Wert der kontinuierlichen Spalte selbst.</span><span class="sxs-lookup"><span data-stu-id="66af5-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="66af5-127">Wenn beim Überarbeiten des vorhersagbaren Spaltensatzes mehr als 500 Ausgabeneuronen generiert werden, generiert [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] im Miningmodell ein Netzwerk zur Darstellung der zusätzlichen Ausgabeneuronen.</span><span class="sxs-lookup"><span data-stu-id="66af5-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="66af5-128">Ein Neuron empfängt Eingaben von anderen Neuronen oder von anderen Daten, je nachdem auf welcher Ebene des Netzwerks es sich befindet.</span><span class="sxs-lookup"><span data-stu-id="66af5-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="66af5-129">Ein Eingabeneuron empfängt Eingaben von den ursprünglichen Daten.</span><span class="sxs-lookup"><span data-stu-id="66af5-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="66af5-130">Verborgene Neuronen und Ausgabeneuronen empfangen Eingaben von den Ausgaben anderer Neuronen im neuronalen Netzwerk.</span><span class="sxs-lookup"><span data-stu-id="66af5-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="66af5-131">Eingaben bilden die Beziehungen zwischen Neuronen, und die Beziehungen dienen als Analysepfad für einen bestimmten Satz von Fällen.</span><span class="sxs-lookup"><span data-stu-id="66af5-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="66af5-132">Jeder Eingabe ist der Wert *Gewichtung*zugeordnet. Dieser beschreibt die Relevanz oder Wichtigkeit einer bestimmten Eingabe an das verborgene oder Ausgabeneuron.</span><span class="sxs-lookup"><span data-stu-id="66af5-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="66af5-133">Je größer die Gewichtung ist, die einer Eingabe zugewiesen wird, desto relevanter oder wichtiger ist der Wert dieser Eingabe.</span><span class="sxs-lookup"><span data-stu-id="66af5-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="66af5-134">Gewichtungen können negativ sein, was bedeutet, dass ein bestimmtes Neuron durch die Eingabe eher deaktiviert als aktiviert werden kann.</span><span class="sxs-lookup"><span data-stu-id="66af5-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="66af5-135">Der Wert jeder Eingabe wird durch die Gewichtung vervielfacht, um die Wichtigkeit einer Eingabe für ein bestimmtes Neuron zu betonen.</span><span class="sxs-lookup"><span data-stu-id="66af5-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="66af5-136">Bei negativen Gewichtungen besteht der Effekt der Vervielfachung des Werts durch die Gewichtung in der Minderung der Wichtigkeit.</span><span class="sxs-lookup"><span data-stu-id="66af5-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="66af5-137">Dementsprechend ist jedem Neuron die einfache nichtlineare *Aktivierungsfunktion*zugeordnet. Diese beschreibt die Relevanz oder die Wichtigkeit eines bestimmten Neurons für die entsprechende Ebene eines neuronalen Netzwerks.</span><span class="sxs-lookup"><span data-stu-id="66af5-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="66af5-138">Als Aktivierungsfunktion wird bei verborgenen Neuronen eine *hypertangentiale Funktion* (tanh) verwendet, bei Ausgabeneuronen dagegen eine *sigmoidale* Funktion.</span><span class="sxs-lookup"><span data-stu-id="66af5-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="66af5-139">Bei beiden Funktionen handelt es sich um nicht nichtlineare, kontinuierliche Funktionen, die im neuronalen Netzwerk die Modellierung von nichtlinearen Beziehungen zwischen Eingabe- und Ausgabeneuronen ermöglichen.</span><span class="sxs-lookup"><span data-stu-id="66af5-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="66af5-140">Trainieren von neuronalen Netzwerken</span><span class="sxs-lookup"><span data-stu-id="66af5-140">Training Neural Networks</span></span>  
 <span data-ttu-id="66af5-141">Das Trainieren eines Data Mining-Modells, das den [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network-Algorithmus verwendet, setzt sich aus mehreren Schritten zusammen:</span><span class="sxs-lookup"><span data-stu-id="66af5-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="66af5-142">Diese Schritte hängen stark von den Werten ab, die Sie für die Algorithmusparameter festlegen.</span><span class="sxs-lookup"><span data-stu-id="66af5-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="66af5-143">Der Algorithmus führt zunächst eine Auswertung aus und extrahiert die Trainingsdaten aus der Datenquelle.</span><span class="sxs-lookup"><span data-stu-id="66af5-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="66af5-144">Ein bestimmter Prozentsatz der Trainingsdaten, die als *zurückgehaltene Daten*bezeichnet werden, ist für die Messung der Genauigkeit des Netzwerks reserviert.</span><span class="sxs-lookup"><span data-stu-id="66af5-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="66af5-145">Während des Trainingsprozesses wird das Netzwerk unmittelbar nach jeder Iteration der Trainingsdaten ausgewertet.</span><span class="sxs-lookup"><span data-stu-id="66af5-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="66af5-146">Wenn die Genauigkeit des Modells nicht mehr zunimmt, wird der Trainingsprozess beendet.</span><span class="sxs-lookup"><span data-stu-id="66af5-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="66af5-147">Die Werte der Parameter *SAMPLE_SIZE* und *HOLDOUT_PERCENTAGE* werden zur Bestimmung der Anzahl von Fällen für die Stichprobe der Trainingsdaten verwendet. Des Weiteren werden sie zur Bestimmung der Anzahl von Fällen verwendet, die für die zurückgehaltenen Daten reserviert werden.</span><span class="sxs-lookup"><span data-stu-id="66af5-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="66af5-148">Der Wert des Parameters *HOLDOUT_SEED* wird verwendet, um die einzelnen Fälle zufällig zu bestimmen, die für die auszunehmen Daten reserviert werden.</span><span class="sxs-lookup"><span data-stu-id="66af5-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="66af5-149">Diese Algorithmusparameter unterscheiden sich von den Eigenschaften HOLDOUT_SIZE und HOLDOUT_SEED, die für eine Miningstruktur zum Definieren eines Testdatasets angewendet werden.</span><span class="sxs-lookup"><span data-stu-id="66af5-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="66af5-150">Der Algorithmus bestimmt als Nächstes die Anzahl und die Komplexität der Netzwerke, die das Miningmodell unterstützt.</span><span class="sxs-lookup"><span data-stu-id="66af5-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="66af5-151">Wenn das Miningmodell mindestens ein Attribut enthält, das nur für die Vorhersage verwendet wird, erstellt der Algorithmus ein einzelnes Netzwerk, in dem jedes dieser Attribute dargestellt wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="66af5-152">Wenn das Miningmodell mindestens ein Attribut enthält, das sowohl für die Eingabe als auch für die Vorhersage verwendet wird, erstellt der Algorithmusanbieter ein Netzwerk für jedes dieser Attribute.</span><span class="sxs-lookup"><span data-stu-id="66af5-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="66af5-153">Bei Eingabe- und vorhersagbaren Attributen mit diskreten Werten stellt jedes Eingabe- oder Ausgabeneuron jeweils einen einzelnen Status dar.</span><span class="sxs-lookup"><span data-stu-id="66af5-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="66af5-154">Bei Eingabe- und vorhersagbaren Attributen mit kontinuierlichen Werten stellt jedes Eingabe- oder Ausgabeneuron jeweils den Bereich und die Verteilung der Werte für das Attribut dar.</span><span class="sxs-lookup"><span data-stu-id="66af5-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="66af5-155">Die maximale Anzahl der Status, die in beiden Fällen unterstützt wird, hängt vom Wert des Algorithmusparameters *MAXIMUM_STATES* ab.</span><span class="sxs-lookup"><span data-stu-id="66af5-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="66af5-156">Wenn die Anzahl der Status eines bestimmten Attributs den Wert des Algorithmusparameters *MAXIMUM_STATES* übersteigt, werden die gebräuchlichsten und relevantesten Status bis zum Maximum der zulässigen Status für dieses Attribut ausgewählt. Die restlichen Status werden als fehlende Werte für die Analyse gruppiert.</span><span class="sxs-lookup"><span data-stu-id="66af5-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="66af5-157">Der Algorithmus verwendet dann beim Bestimmen der ersten Anzahl an Neuronen, die zum Erstellen der verborgenen Ebene verwendet werden, den Wert des Parameters *HIDDEN_NODE_RATIO* .</span><span class="sxs-lookup"><span data-stu-id="66af5-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="66af5-158">Sie können den Parameter *HIDDEN_NODE_RATIO* auf 0 festlegen, um zu verhindern, dass in den Netzwerken eine verborgene Ebene erstellt wird, die der Algorithmus für das Miningmodell generiert. Somit wird sichergestellt, dass das neuronale Netzwerk als logistische Regression behandelt wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="66af5-159">Der Algorithmusanbieter führt zum selben Zeitpunkt die iterative Auswertung der Gewichtung aller Eingaben im gesamten Netzwerk aus. Dabei vergleicht er den zuvor reservierten Trainingsdatensatz mit den tatsächlich bekannten Werten aller Fälle der zurückgehaltenen Daten mit der Vorhersage des Netzwerks in einem Prozess, der als *Batchlernvorgang*bezeichnet wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="66af5-160">Nach der Auswertung des gesamten Trainingsdatensatzes vergleicht der Algorithmus für jedes Neuron den vorhergesagten Wert mit dem Ist-Wert.</span><span class="sxs-lookup"><span data-stu-id="66af5-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="66af5-161">Der Algorithmus berechnet bei Bedarf den Fehlergrad und passt die Gewichtungen an, die den Eingaben dieses Neurons zugeordnet sind. Dabei arbeitet der Algorithmus rückwärts von den Ausgabeneuronen ausgehend zu den Eingabeneuronen mithilfe eines Prozesses, der als *Rückpropagierung*bezeichnet wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="66af5-162">Der Algorithmus wiederholt dann diesen Prozess für den gesamten Trainingsdatensatz.</span><span class="sxs-lookup"><span data-stu-id="66af5-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="66af5-163">Da der Algorithmus viele Gewichtungen und Ausgabeneuronen unterstützen kann, wird der Algorithmus der konjugierten Gradienten verwendet, um den Trainingsprozess beim Zuweisen und Auswerten der Eingabegewichtungen zu führen.</span><span class="sxs-lookup"><span data-stu-id="66af5-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="66af5-164">Eine Erläuterung des Algorithmus der konjugierten Gradienten geht über den Rahmen dieser Dokumentation hinaus.</span><span class="sxs-lookup"><span data-stu-id="66af5-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="66af5-165">Featureauswahl</span><span class="sxs-lookup"><span data-stu-id="66af5-165">Feature Selection</span></span>  
 <span data-ttu-id="66af5-166">Wenn die Anzahl der Eingabeattribute größer ist als der Wert des *MAXIMUM_INPUT_ATTRIBUTES* -Parameters oder die Anzahl der vorhersagbaren Attribute größer ist als der Wert des *MAXIMUM_OUTPUT_ATTRIBUTES* -Parameters, wird für die Funktionsauswahl ein entsprechender Algorithmus verwendet, um die Komplexität der im Miningmodell eingeschlossenen Netzwerke zu reduzieren.</span><span class="sxs-lookup"><span data-stu-id="66af5-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="66af5-167">Mit der Funktionsauswahl wird die Anzahl der Eingabe- und vorhersagbaren Attribute reduziert. Dabei werden die Attribute beibehalten, die für das Modell statistisch am wichtigsten sind.</span><span class="sxs-lookup"><span data-stu-id="66af5-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="66af5-168">Die Funktionsauswahl wird automatisch von allen [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] Data Mining-Algorithmen zur Verbesserung der Analyse und zur Reduzierung der Verarbeitungslast verwendet.</span><span class="sxs-lookup"><span data-stu-id="66af5-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="66af5-169">Die für die Funktionsauswahl in neuronalen Netzwerkmodellen verwendete Methode hängt vom Datentyp des Attributs ab.</span><span class="sxs-lookup"><span data-stu-id="66af5-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="66af5-170">Zu Referenzzwecken zeigt die folgende Tabelle die für neuronale Netzwerkmodelle verwendeten Funktionsauswahlmethoden. Außerdem enthält die Tabelle die für den logistischen Regressionsalgorithmus verwendeten Funktionsauswahlmethoden, die auf dem neuronalen Netzwerkalgorithmus basieren.</span><span class="sxs-lookup"><span data-stu-id="66af5-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="66af5-171">Algorithmus</span><span class="sxs-lookup"><span data-stu-id="66af5-171">Algorithm</span></span>|<span data-ttu-id="66af5-172">Analysemethode</span><span class="sxs-lookup"><span data-stu-id="66af5-172">Method of analysis</span></span>|<span data-ttu-id="66af5-173">Kommentare</span><span class="sxs-lookup"><span data-stu-id="66af5-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="66af5-174">Neuronale Netzwerke</span><span class="sxs-lookup"><span data-stu-id="66af5-174">Neural Network</span></span>|<span data-ttu-id="66af5-175">Interessantheitsgrad</span><span class="sxs-lookup"><span data-stu-id="66af5-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="66af5-176">Shannon-Entropie</span><span class="sxs-lookup"><span data-stu-id="66af5-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="66af5-177">Bayes-Methode mit K2-A-priori-Verteilung</span><span class="sxs-lookup"><span data-stu-id="66af5-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="66af5-178">Bayes-Dirichlet mit uniformer A-priori-Verteilung (Standard)</span><span class="sxs-lookup"><span data-stu-id="66af5-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="66af5-179">Der Neural Networks-Algorithmus kann sowohl die Entropie- als auch die Bayes-basierte Bewertungsmethode verwenden, sofern die Daten kontinuierliche Spalten enthalten.</span><span class="sxs-lookup"><span data-stu-id="66af5-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="66af5-180">Standard.</span><span class="sxs-lookup"><span data-stu-id="66af5-180">Default.</span></span>|  
|<span data-ttu-id="66af5-181">Logistische Regression</span><span class="sxs-lookup"><span data-stu-id="66af5-181">Logistic Regression</span></span>|<span data-ttu-id="66af5-182">Interessantheitsgrad</span><span class="sxs-lookup"><span data-stu-id="66af5-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="66af5-183">Shannon-Entropie</span><span class="sxs-lookup"><span data-stu-id="66af5-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="66af5-184">Bayes-Methode mit K2-A-priori-Verteilung</span><span class="sxs-lookup"><span data-stu-id="66af5-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="66af5-185">Bayes-Dirichlet mit uniformer A-priori-Verteilung (Standard)</span><span class="sxs-lookup"><span data-stu-id="66af5-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="66af5-186">Weil diesem Algorithmus kein Parameter zur Steuerung des Funktionsauswahlverhaltens übergeben werden kann, werden die Standardwerte verwendet.</span><span class="sxs-lookup"><span data-stu-id="66af5-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="66af5-187">Wenn alle Attribute diskret oder diskretisiert sind, wird als Standardmethode Bayes-Dirichlet mit uniformer A-priori-Verteilung eingesetzt.</span><span class="sxs-lookup"><span data-stu-id="66af5-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="66af5-188">Die Algorithmusparameter, die die Funktionsauswahl für ein neuronales Netzwerkmodell steuern, sind MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES und MAXIMUM_STATES.</span><span class="sxs-lookup"><span data-stu-id="66af5-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="66af5-189">Sie können auch die Anzahl von verborgenen Ebenen kontrollieren, indem Sie den HIDDEN_NODE_RATIO-Parameter festlegen.</span><span class="sxs-lookup"><span data-stu-id="66af5-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="66af5-190">Bewertungsmethoden</span><span class="sxs-lookup"><span data-stu-id="66af5-190">Scoring Methods</span></span>  
 <span data-ttu-id="66af5-191">Bei der*Bewertung* handelt es sich um eine Form der Normalisierung, was im Trainingskontext eines neuronalen Netzwerks den Prozess bezeichnet, mit dem ein Wert, z. B. eine diskrete Textbeschriftung, in einen Wert konvertiert wird, der mit anderen Eingaben verglichen und im Netzwerk gewichtet werden kann.</span><span class="sxs-lookup"><span data-stu-id="66af5-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="66af5-192">Wenn ein Eingabeattribut beispielsweise Gender ist und die möglichen Werte Male und Female sind, während ein anderes Eingabeattribut Income lautet mit einem variablen Wertebereich, sind die Werte für jedes Attribut nicht direkt vergleichbar und müssen auf eine gemeinsame Skalierung codiert werden, sodass die Gewichtungen berechnet werden können.</span><span class="sxs-lookup"><span data-stu-id="66af5-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="66af5-193">Bei der Bewertung werden diese Eingaben zu numerischen Werten normalisiert, insbesondere zu einem Wahrscheinlichkeitsbereich.</span><span class="sxs-lookup"><span data-stu-id="66af5-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="66af5-194">Die für die Normalisierung verwendeten Funktionen tragen auch zu einer gleichmäßigeren Verteilung des Eingabewerts auf einer einheitlichen Skala bei, sodass Extremwerte die Analyseergebnisse nicht beeinträchtigen.</span><span class="sxs-lookup"><span data-stu-id="66af5-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="66af5-195">Ausgaben des neuronalen Netzwerks werden ebenfalls codiert.</span><span class="sxs-lookup"><span data-stu-id="66af5-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="66af5-196">Wenn es ein einzelnes Ziel für die Ausgabe (d. h. Vorhersage) gibt oder mehrere Ziele vorhanden sind, die nur für die Vorhersage und nicht für die Eingabe verwendet werden, erstellt das Modell ein einzelnes Netzwerk, und eine Normalisierung der Werte erscheint nicht erforderlich.</span><span class="sxs-lookup"><span data-stu-id="66af5-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="66af5-197">Wenn mehrere Attribute für die Eingabe und die Vorhersage verwendet werden, muss das Modell mehrere Netzwerke erstellen. Daher müssen alle Werte normalisiert sein, und die Ausgaben müssen beim Verlassen des Netzwerks codiert werden.</span><span class="sxs-lookup"><span data-stu-id="66af5-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="66af5-198">Die Codierung der Eingaben basiert auf der Summierung jedes einzelnen diskreten Werts im Trainingsfall und Multiplizierung dieses Werts mit der Gewichtung.</span><span class="sxs-lookup"><span data-stu-id="66af5-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="66af5-199">Dies wird als *gewichtete Summe*bezeichnet, die an die Aktivierungsfunktion in der verborgenen Ebene übergeben wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="66af5-200">Ein z-Ergebnis wird wie folgt zur Codierung verwendet:</span><span class="sxs-lookup"><span data-stu-id="66af5-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="66af5-201">**Diskrete Werte**</span><span class="sxs-lookup"><span data-stu-id="66af5-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="66af5-202">° = p-die vorherige Wahrscheinlichkeit eines Zustands</span><span class="sxs-lookup"><span data-stu-id="66af5-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="66af5-203">STDDEV = sqrt (p (1-p))</span><span class="sxs-lookup"><span data-stu-id="66af5-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="66af5-204">**Kontinuierliche Werte**</span><span class="sxs-lookup"><span data-stu-id="66af5-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="66af5-205">Vorhandener Wert = 1-°/Wert</span><span class="sxs-lookup"><span data-stu-id="66af5-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="66af5-206">Kein vorhandener Wert =-"/"</span><span class="sxs-lookup"><span data-stu-id="66af5-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="66af5-207">Nachdem die Werte codiert wurden, durchlaufen die Eingaben die gewichtete Summierung, wobei die Netzwerkränder als Gewichtungen fungieren.</span><span class="sxs-lookup"><span data-stu-id="66af5-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="66af5-208">Bei der Codierung für Ausgaben wird die sigmoidale Funktion verwendet, die über für Vorhersagen nützliche Eigenschaften verfügt.</span><span class="sxs-lookup"><span data-stu-id="66af5-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="66af5-209">Eine dieser Eigenschaften besteht darin, dass die Ausgabe dieser Funktion unabhängig davon, wie die ursprünglichen Werte skaliert sind und ob die Werte negativ oder positiv sind, stets einen Wert zwischen 0 und 1 aufweist. Dies eignet sich für die Schätzung von Wahrscheinlichkeiten.</span><span class="sxs-lookup"><span data-stu-id="66af5-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="66af5-210">Eine andere nützliche Eigenschaft besteht darin, dass die sigmoidale Funktion einen glättenden Effekt hat, sodass die Wahrscheinlichkeit mit zunehmender Entfernung der Werte vom Wendepunkt langsam gegen 0 oder 1 gehen.</span><span class="sxs-lookup"><span data-stu-id="66af5-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="66af5-211">Anpassen des Neural Network-Algorithmus</span><span class="sxs-lookup"><span data-stu-id="66af5-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="66af5-212">Der [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network-Algorithmus unterstützt mehrere Parameter, die Auswirkungen auf das Verhalten, die Leistung und die Genauigkeit des resultierenden Miningmodells haben.</span><span class="sxs-lookup"><span data-stu-id="66af5-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="66af5-213">Sie können die Verarbeitungsweise der Daten auch durch Festlegen der Modellierungsflags für die Spalten oder durch Festlegen von Verteilungsflags ändern, um anzugeben, wie die Werte innerhalb der Spalten behandelt werden sollen.</span><span class="sxs-lookup"><span data-stu-id="66af5-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="66af5-214">Festlegen von Algorithmusparametern</span><span class="sxs-lookup"><span data-stu-id="66af5-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="66af5-215">In der folgenden Tabelle werden die Parameter beschrieben, die mit dem Microsoft Neural Network-Algorithmus verwendet werden können.</span><span class="sxs-lookup"><span data-stu-id="66af5-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="66af5-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="66af5-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="66af5-217">Legt das Verhältnis von verborgenen Neuronen zu Eingabe- und Ausgabeneuronen fest.</span><span class="sxs-lookup"><span data-stu-id="66af5-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="66af5-218">Die folgende Formel bestimmt die erste Anzahl von Neuronen in der verborgenen Ebene:</span><span class="sxs-lookup"><span data-stu-id="66af5-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="66af5-219">HIDDEN_NODE_RATIO \* SQRT(Gesamtzahl der Eingabeneuronen \* Gesamtzahl der Ausgabeneuronen)</span><span class="sxs-lookup"><span data-stu-id="66af5-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="66af5-220">Der Standardwert ist 4.0.</span><span class="sxs-lookup"><span data-stu-id="66af5-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="66af5-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="66af5-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="66af5-222">Gibt den Prozentsatz von Fällen in den Trainingsdaten an, die zum Berechnen des Fehlers für zurückgehaltene Daten verwendet werden. Dieser dient als Teil des Beendigungskriteriums beim Trainieren des Miningmodells.</span><span class="sxs-lookup"><span data-stu-id="66af5-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="66af5-223">Der Standardwert ist 30.</span><span class="sxs-lookup"><span data-stu-id="66af5-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="66af5-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="66af5-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="66af5-225">Gibt eine Zahl an, die als Ausgangswert für den Pseudozufallszahlen-Generator zum zufälligen Generieren von zurückgehaltenen Daten verwendet wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="66af5-226">Wenn dieser Parameter auf 0 festgelegt ist, wird der Ausgangswert vom Algorithmus basierend auf dem Namen des Miningmodells generiert. So wird sichergestellt, dass der Inhalt bei erneuter Verarbeitung des Modells gleich bleibt.</span><span class="sxs-lookup"><span data-stu-id="66af5-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="66af5-227">Der Standardwert ist 0.</span><span class="sxs-lookup"><span data-stu-id="66af5-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="66af5-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="66af5-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="66af5-229">Gibt die maximale Anzahl von Eingabeattributen an, die an den Algorithmus übergeben werden kann, bevor die Funktionsauswahl verwendet wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="66af5-230">Wenn dieser Wert auf 0 festgelegt wird, ist die Funktionsauswahl für Eingabeattribute deaktiviert.</span><span class="sxs-lookup"><span data-stu-id="66af5-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="66af5-231">Der Standardwert ist 255.</span><span class="sxs-lookup"><span data-stu-id="66af5-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="66af5-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="66af5-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="66af5-233">Gibt die maximale Anzahl von Ausgabeattributen an, die an den Algorithmus übergeben werden kann, bevor die Funktionsauswahl verwendet wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="66af5-234">Wenn dieser Wert auf 0 festgelegt wird, ist die Funktionsauswahl für Ausgabeattribute deaktiviert.</span><span class="sxs-lookup"><span data-stu-id="66af5-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="66af5-235">Der Standardwert ist 255.</span><span class="sxs-lookup"><span data-stu-id="66af5-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="66af5-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="66af5-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="66af5-237">Gibt die maximale Anzahl an diskreten Status pro Attribut an, die vom Algorithmus unterstützt wird.</span><span class="sxs-lookup"><span data-stu-id="66af5-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="66af5-238">Wenn die Anzahl der Status eines bestimmten Attributs größer ist als die für den Parameter festgelegte Statusanzahl, verwendet der Algorithmus für dieses Attribut die gebräuchlichsten Status und behandelt die restlichen Status als fehlend.</span><span class="sxs-lookup"><span data-stu-id="66af5-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="66af5-239">Der Standardwert ist 100.</span><span class="sxs-lookup"><span data-stu-id="66af5-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="66af5-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="66af5-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="66af5-241">Gibt die Anzahl von Fällen an, die zum Trainieren des Modells verwendet werden.</span><span class="sxs-lookup"><span data-stu-id="66af5-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="66af5-242">Der Algorithmus verwendet entweder diese Anzahl oder den Prozentsatz aller Fälle, die – wie im HOLDOUT_PERCENTAGE-Parameter angegeben – nicht in den zurückgehaltenen Daten eingeschlossen sind, je nachdem, welcher Wert kleiner ist.</span><span class="sxs-lookup"><span data-stu-id="66af5-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="66af5-243">Mit anderen Worten, wenn der HOLDOUT_PERCENTAGE-Parameter auf 30 festgelegt ist, verwendet der Algorithmus entweder den Wert dieses Parameters oder einen Wert, der bis zu 70 % gleich der Gesamtzahl der Fälle ist, je nachdem, welcher Wert kleiner ist.</span><span class="sxs-lookup"><span data-stu-id="66af5-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="66af5-244">Der Standardwert ist 10.000.</span><span class="sxs-lookup"><span data-stu-id="66af5-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="66af5-245">Modellierungsflags</span><span class="sxs-lookup"><span data-stu-id="66af5-245">Modeling Flags</span></span>  
 <span data-ttu-id="66af5-246">Die folgenden Modellierungsflags werden zur Verwendung mit dem [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network-Algorithmus unterstützt.</span><span class="sxs-lookup"><span data-stu-id="66af5-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="66af5-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="66af5-247">NOT NULL</span></span>  
 <span data-ttu-id="66af5-248">Gibt an, dass die Spalte keinen NULL-Wert enthalten kann.</span><span class="sxs-lookup"><span data-stu-id="66af5-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="66af5-249">Ein Fehler tritt auf, wenn Analysis Services während des Modelltrainings einen NULL-Wert erkennt.</span><span class="sxs-lookup"><span data-stu-id="66af5-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="66af5-250">Gilt für die Miningstrukturspalten.</span><span class="sxs-lookup"><span data-stu-id="66af5-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="66af5-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="66af5-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="66af5-252">Gibt an, dass das Modell nur berücksichtigen soll, ob ein Wert für das Attribut vorhanden ist oder ob ein Wert fehlt.</span><span class="sxs-lookup"><span data-stu-id="66af5-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="66af5-253">Der genaue Wert spielt keine Rolle.</span><span class="sxs-lookup"><span data-stu-id="66af5-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="66af5-254">Gilt für die Miningmodellspalten.</span><span class="sxs-lookup"><span data-stu-id="66af5-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="66af5-255">Verteilungsflags</span><span class="sxs-lookup"><span data-stu-id="66af5-255">Distribution Flags</span></span>  
 <span data-ttu-id="66af5-256">Die folgenden Verteilungsflags werden zur Verwendung mit dem [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network-Algorithmus unterstützt.</span><span class="sxs-lookup"><span data-stu-id="66af5-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="66af5-257">Die Flags dienen nur als Hinweise für das Modell. Wenn der Algorithmus eine andere Verteilung erkennt, wird die gefundene Verteilung und nicht die von dem Hinweis zur Verfügung gestellte Verteilung verwendet.</span><span class="sxs-lookup"><span data-stu-id="66af5-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="66af5-258">Normal</span><span class="sxs-lookup"><span data-stu-id="66af5-258">Normal</span></span>  
 <span data-ttu-id="66af5-259">Gibt an, dass die Werte innerhalb der Spalte so behandelt werden sollen, als würden sie die normale oder Gauß'sche Verteilung darstellen.</span><span class="sxs-lookup"><span data-stu-id="66af5-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="66af5-260">Uniform</span><span class="sxs-lookup"><span data-stu-id="66af5-260">Uniform</span></span>  
 <span data-ttu-id="66af5-261">Gibt an, dass die Werte innerhalb der Spalte so behandelt werden sollen, als wären sie gleichmäßig verteilt. Das heißt, die Wahrscheinlichkeit der einzelnen Werte ist ungefähr gleich und ist eine Funktion der Gesamtzahl der Werte.</span><span class="sxs-lookup"><span data-stu-id="66af5-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="66af5-262">Log Normal</span><span class="sxs-lookup"><span data-stu-id="66af5-262">Log Normal</span></span>  
 <span data-ttu-id="66af5-263">Gibt an, dass die Werte innerhalb der Spalte so behandelt werden sollen, als wären sie entlang der *Protokollnormalkurve* verteilt. Dies bedeutet, dass der Logarithmus der Werte normal verteilt ist.</span><span class="sxs-lookup"><span data-stu-id="66af5-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="66af5-264">Requirements (Anforderungen)</span><span class="sxs-lookup"><span data-stu-id="66af5-264">Requirements</span></span>  
 <span data-ttu-id="66af5-265">Ein neuronales Netzwerkmodell muss mindestens eine Eingabespalte und eine Ausgabespalte enthalten.</span><span class="sxs-lookup"><span data-stu-id="66af5-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="66af5-266">Eingabespalten und vorhersagbare Spalten</span><span class="sxs-lookup"><span data-stu-id="66af5-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="66af5-267">Der [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network-Algorithmus unterstützt bestimmte Eingabespalten und vorhersagbare Spalten. Diese sind in der nachstehenden Tabelle aufgelistet.</span><span class="sxs-lookup"><span data-stu-id="66af5-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="66af5-268">Spalte</span><span class="sxs-lookup"><span data-stu-id="66af5-268">Column</span></span>|<span data-ttu-id="66af5-269">Inhaltstypen</span><span class="sxs-lookup"><span data-stu-id="66af5-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="66af5-270">Eingabeattribut</span><span class="sxs-lookup"><span data-stu-id="66af5-270">Input attribute</span></span>|<span data-ttu-id="66af5-271">Continuous, Cyclical, Discrete, Discretized, Key, Table und Ordered</span><span class="sxs-lookup"><span data-stu-id="66af5-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="66af5-272">Vorhersagbares Attribut</span><span class="sxs-lookup"><span data-stu-id="66af5-272">Predictable attribute</span></span>|<span data-ttu-id="66af5-273">Continuous, Cyclical, Discrete, Discretized und Ordered</span><span class="sxs-lookup"><span data-stu-id="66af5-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="66af5-274">Zyklische und sortierte Inhaltstypen werden unterstützt, der Algorithmus behandelt sie jedoch als diskrete Werte und führt keine spezielle Verarbeitung durch.</span><span class="sxs-lookup"><span data-stu-id="66af5-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="66af5-275">Weitere Informationen</span><span class="sxs-lookup"><span data-stu-id="66af5-275">See Also</span></span>  
 <span data-ttu-id="66af5-276">[Microsoft Neural Network-Algorithmus](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="66af5-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="66af5-277">[Mining Modell Inhalt von neuronalen Netzwerkmodellen &#40;Analysis Services Data Mining-&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="66af5-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="66af5-278">Neuronale Beispiele für Netzwerkmodellabfragen</span><span class="sxs-lookup"><span data-stu-id="66af5-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
