---
title: Funktionsauswahl (Data Mining) | Microsoft-Dokumentation
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: de-DE
ms.lasthandoff: 08/04/2020
ms.locfileid: "87619453"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="a5763-102">Funktionsauswahl (Data Mining)</span><span class="sxs-lookup"><span data-stu-id="a5763-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="a5763-103">Die *Featureauswahl* ist ein Begriff, der in Data Mining häufig verwendet wird, um die Tools und Techniken zu beschreiben, die für die Reduzierung von Eingaben auf eine verwaltbare Größe für die Verarbeitung</span><span class="sxs-lookup"><span data-stu-id="a5763-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="a5763-104">Die Featureauswahl impliziert nicht nur die *kardinalitätsreduzierung*. Dies bedeutet, dass ein beliebiges oder vordefiniertes Umstellungs Verhalten für die Anzahl der Attribute, die bei der Erstellung eines Modells berücksichtigt werden können, auferlegt wird, aber auch die Auswahl der Attribute, d. h., dass der Analyst oder das Modellierungs Tool basierend auf der Nützlichkeit der Analyse aktiv Attribute auswählt</span><span class="sxs-lookup"><span data-stu-id="a5763-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="a5763-105">Die Möglichkeit, eine Funktionsauswahl anzuwenden, ist wichtig für eine effiziente Analyse, da Datasets häufig wesentlich mehr Informationen enthalten, als für die Modellerstellung erforderlich ist.</span><span class="sxs-lookup"><span data-stu-id="a5763-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="a5763-106">Ein Dataset kann z. B. 500 Spalten mit Kundenmerkmalen enthalten. Wenn die Daten in einigen Spalten jedoch nur einen geringen Informationswert haben, würden diese Spalten, wenn sie dem Modell hinzugefügt würden, nur einen sehr geringen Nutzen bringen.</span><span class="sxs-lookup"><span data-stu-id="a5763-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="a5763-107">Wenn Sie die nicht verwendeten Spalten beim Erstellen des Modells beibehalten, ist während des Trainingsprozesses mehr CPU und Arbeitsspeicher erforderlich, und das fertige Modell erfordert mehr Speicherplatz.</span><span class="sxs-lookup"><span data-stu-id="a5763-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="a5763-108">Auch wenn die Ressourcen kein Problem sind, empfiehlt es sich, nicht verwendete Spalten zu entfernen, da sie die Qualität der erkannten Muster aus folgenden Gründen beeinträchtigen können:</span><span class="sxs-lookup"><span data-stu-id="a5763-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="a5763-109">Manche Spalten enthalten stark abweichende oder redundante Werte.</span><span class="sxs-lookup"><span data-stu-id="a5763-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="a5763-110">Dies erschwert es, in den Daten sinnvolle Muster zu erkennen.</span><span class="sxs-lookup"><span data-stu-id="a5763-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="a5763-111">Zur Erkennung qualitativ hochwertiger Muster benötigen die meisten Data Mining-Algorithmen ein viel größeres Trainingsdataset für mehrdimensionale Datasets.</span><span class="sxs-lookup"><span data-stu-id="a5763-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="a5763-112">In einigen Data Mining-Anwendungen ist das Trainingsdataset jedoch sehr klein.</span><span class="sxs-lookup"><span data-stu-id="a5763-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="a5763-113">Wenn von den 500 Spalten in der Datenquelle nur 50 Spalten Informationen enthalten, die bei der Modellerstellung von Nutzen sind, könnten Sie sie einfach nicht in das Modell einbeziehen. Alternativ könnten Sie mithilfe der Funktionsauswahl automatisch die besten Funktionen ermitteln und Werte ausschließen, die statistisch unbedeutend sind.</span><span class="sxs-lookup"><span data-stu-id="a5763-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="a5763-114">Die Funktionsauswahl ist hilfreich bei der Lösung des doppelten Problems, dass zu viele Daten mit geringem Wert oder zu wenige hochwertige Daten vorhanden sind.</span><span class="sxs-lookup"><span data-stu-id="a5763-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="a5763-115">Die Funktionsauswahl bei Analysis Services Data Mining</span><span class="sxs-lookup"><span data-stu-id="a5763-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="a5763-116">Normalerweise wird die Funktionsauswahl in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] automatisch ausgeführt. Jeder Algorithmus verfügt über eine Reihe von Standardtechniken für die intelligente Anwendung der Funktionsreduzierung.</span><span class="sxs-lookup"><span data-stu-id="a5763-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="a5763-117">Die Funktionsauswahl wird stets durchgeführt, bevor das Modell trainiert wird, um automatisch die Attribute in einem Dataset auszuwählen, die im Modell am wahrscheinlichsten Verwendung finden.</span><span class="sxs-lookup"><span data-stu-id="a5763-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="a5763-118">Sie können jedoch auch manuell Parameter festlegen, um das Verhalten der Funktionsauswahl zu beeinflussen.</span><span class="sxs-lookup"><span data-stu-id="a5763-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="a5763-119">Im Allgemeinen wird bei der Funktionsauswahl ein Wert für jedes Attribut berechnet, und dann werden nur diejenigen Attribute ausgewählt, die über die besten Werte verfügen.</span><span class="sxs-lookup"><span data-stu-id="a5763-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="a5763-120">Sie können den Schwellenwert für die besten Ergebnisse auch anpassen.</span><span class="sxs-lookup"><span data-stu-id="a5763-120">You can also adjust the threshold for the top scores.</span></span> [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] <span data-ttu-id="a5763-121">bietet mehrere Methoden zum Berechnen dieser Ergebnisse. Welche spezifische Methode auf das jeweilige Modell angewendet wird, hängt von folgenden Faktoren ab:</span><span class="sxs-lookup"><span data-stu-id="a5763-121">provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="a5763-122">Im Modell verwendeter Algorithmus</span><span class="sxs-lookup"><span data-stu-id="a5763-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="a5763-123">Datentyp des Attributs</span><span class="sxs-lookup"><span data-stu-id="a5763-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="a5763-124">Für das Modell festgelegte Parameter</span><span class="sxs-lookup"><span data-stu-id="a5763-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="a5763-125">Die Funktionsauswahl wird auf Eingaben, vorhersagbare Attribute und Statuswerte in einer Spalte angewandt.</span><span class="sxs-lookup"><span data-stu-id="a5763-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="a5763-126">Nachdem die Bewertung für die Funktionsauswahl abgeschlossen ist, werden nur die vom Algorithmus ausgewählten Attribute und Statusangaben bei der Modellerstellung berücksichtigt und für Vorhersagen zur Verfügung gestellt.</span><span class="sxs-lookup"><span data-stu-id="a5763-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="a5763-127">Bei Auswahl eines vorhersagbaren Attributs, das den Schwellenwert für die Funktionsauswahl nicht erfüllt, kann das Attribut zwar trotzdem für die Vorhersage verwendet werden, in diesem Fall basieren die Vorhersagen jedoch ausschließlich auf den globalen, im Modell vorhandenen Statistiken.</span><span class="sxs-lookup"><span data-stu-id="a5763-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="a5763-128">Die Funktionsauswahl betrifft nur die Spalten, die im Modell verwendet werden, und wirkt sich nicht auf die Speicherung der Miningstruktur aus.</span><span class="sxs-lookup"><span data-stu-id="a5763-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="a5763-129">Die Spalten, die nicht in das Miningmodell aufgenommen werden, sind in der Struktur weiterhin verfügbar, und die Daten der Miningstrukturspalten werden zwischengespeichert.</span><span class="sxs-lookup"><span data-stu-id="a5763-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="a5763-130">Definition von Funktionsauswahlmethoden</span><span class="sxs-lookup"><span data-stu-id="a5763-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="a5763-131">Abhängig vom Typ der Daten, mit denen Sie arbeiten, und dem für die Analyse gewählten Algorithmus, kann die Funktionsauswahl auf vielerlei Weise implementiert werden.</span><span class="sxs-lookup"><span data-stu-id="a5763-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="a5763-132">SQL Server Analysis Services stellt mehrere gängige und bekannte Methoden zum Bewerten von Attributen bereit.</span><span class="sxs-lookup"><span data-stu-id="a5763-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="a5763-133">Die Methode, die bei Algorithmen oder Datasets angewendet wird, hängt von den Datentypen und der Spaltenverwendung ab.</span><span class="sxs-lookup"><span data-stu-id="a5763-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="a5763-134">Die Bewertung des *Interessantheitsgrads* wird zum Festlegen der Rangfolge und zum Sortieren von Attributen in Spalten verwendet, die nicht binäre, kontinuierliche, numerische Daten enthalten.</span><span class="sxs-lookup"><span data-stu-id="a5763-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="a5763-135">Für Spalten, die diskrete und diskretisierte Daten enthalten, sind die*Shannon-Entropie* und zwei *Bayes* -Werte verfügbar.</span><span class="sxs-lookup"><span data-stu-id="a5763-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="a5763-136">Wenn das Modell jedoch kontinuierliche Spalten enthält, wird der Interessantheitsgrad zur Bewertung aller Eingabespalten herangezogen, um die Konsistenz sicherzustellen.</span><span class="sxs-lookup"><span data-stu-id="a5763-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="a5763-137">Im folgenden Abschnitt werden die Methoden der Funktionsauswahl einzeln beschrieben.</span><span class="sxs-lookup"><span data-stu-id="a5763-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="a5763-138">Interessantheitsgrad</span><span class="sxs-lookup"><span data-stu-id="a5763-138">Interestingness score</span></span>  
 <span data-ttu-id="a5763-139">Eine Funktion ist interessant, wenn sie eine nützliche Information offenbart.</span><span class="sxs-lookup"><span data-stu-id="a5763-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="a5763-140">Da die Definition der nützlichen variiert, hängt vom jeweiligen Szenario ab. die Data Mining Branche hat verschiedene Möglichkeiten zum Messen der *Interessantheit*entwickelt.</span><span class="sxs-lookup"><span data-stu-id="a5763-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="a5763-141">Beispielsweise könnte die *Neuheit* bei der Ausreißererkennung interessant sein, aber die Möglichkeit zur Unterscheidung zwischen eng verknüpften Elementen oder der *Gewichtungs Gewichtung*kann für die Klassifizierung interessanter sein.</span><span class="sxs-lookup"><span data-stu-id="a5763-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="a5763-142">Das in SQL Server Analysis Services verwendete Maß für Interessantheit ist *Entropie basiert*, d. h., dass Attribute mit zufälligen Verteilungen eine höhere Entropie und einen geringeren Informationsgewinn aufweisen. Daher sind solche Attribute weniger interessant.</span><span class="sxs-lookup"><span data-stu-id="a5763-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="a5763-143">Die Entropie eines bestimmten Attributs wird wie folgt mit der Entropie aller anderen Attribute verglichen:</span><span class="sxs-lookup"><span data-stu-id="a5763-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="a5763-144">Interessantheit(Attribut) = - (m - Entropie(Attribut)) \* (m - Entropie(Attribut))</span><span class="sxs-lookup"><span data-stu-id="a5763-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="a5763-145">Die zentrale Entropie oder m steht für die Entropie des gesamten Featuresatzes.</span><span class="sxs-lookup"><span data-stu-id="a5763-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="a5763-146">Durch Abziehen der Entropie des Zielattributs von der zentralen Entropie lässt sich einschätzen, wie viele Informationen das Attribut bereitstellt.</span><span class="sxs-lookup"><span data-stu-id="a5763-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="a5763-147">Dieses Ergebnis wird standardmäßig immer dann verwendet, wenn die Spalte nicht binäre, kontinuierliche, numerische Daten enthält.</span><span class="sxs-lookup"><span data-stu-id="a5763-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="a5763-148">Shannon-Entropie</span><span class="sxs-lookup"><span data-stu-id="a5763-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="a5763-149">Die Shannon-Entropie stellt ein Maß für die Ungewissheit einer zufälligen Variable für ein bestimmtes Ergebnis dar.</span><span class="sxs-lookup"><span data-stu-id="a5763-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="a5763-150">Beispielsweise kann die Entropie einen Münzwurfs als Funktion der Wahrscheinlichkeit des Ergebnisses "Kopf" dargestellt werden.</span><span class="sxs-lookup"><span data-stu-id="a5763-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="a5763-151">In Analysis Services wird die folgende Formel zur Berechnung der Shannon-Entropie verwendet:</span><span class="sxs-lookup"><span data-stu-id="a5763-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="a5763-152">H(X) = -∑ P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="a5763-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="a5763-153">Diese Bewertungsmethode ist für diskrete und diskretisierte Attribute verfügbar.</span><span class="sxs-lookup"><span data-stu-id="a5763-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="a5763-154">Bayes-Methode mit K2-A-priori-Verteilung</span><span class="sxs-lookup"><span data-stu-id="a5763-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="a5763-155">Analysis Services stellt zwei Funktionsauswahlwerte bereit, die auf Bayes-Netzwerken basieren.</span><span class="sxs-lookup"><span data-stu-id="a5763-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="a5763-156">Ein Bayes-Netzwerk ist ein *gerichteter* oder *azyklischer* Graph von Zuständen und Übergängen zwischen Zuständen. Das heißt, dass einige Zustände immer vor dem aktuellen Status liegen, andere Zustände sind nachgelagert, und der Graph stellt keine Wiederholungen oder Schleifen dar.</span><span class="sxs-lookup"><span data-stu-id="a5763-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="a5763-157">Definitionsgemäß ermöglichen Bayes-Netzwerke die Verwendung vorherigen Wissens.</span><span class="sxs-lookup"><span data-stu-id="a5763-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="a5763-158">Allerdings ist die Frage, welche der früheren Zustände zur Berechnung der Wahrscheinlichkeit nachfolgender Zustände verwendet werden sollen, für den Algorithmusentwurf, die Leistung und die Genauigkeit wichtig.</span><span class="sxs-lookup"><span data-stu-id="a5763-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="a5763-159">Der K2-Algorithmus zum Lernen von Bayes-Netzwerken wurde von Cooper und Herskovits entwickelt und wird häufig im Data Mining eingesetzt.</span><span class="sxs-lookup"><span data-stu-id="a5763-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="a5763-160">Er ist skalierbar und kann mehrere Variablen analysieren, erfordert jedoch eine Sortierung der als Eingabe verwendeten Variablen.</span><span class="sxs-lookup"><span data-stu-id="a5763-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="a5763-161">Weitere Informationen finden Sie in [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) von Chickering, Geiger, und Heckerman.</span><span class="sxs-lookup"><span data-stu-id="a5763-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="a5763-162">Diese Bewertungsmethode ist für diskrete und diskretisierte Attribute verfügbar.</span><span class="sxs-lookup"><span data-stu-id="a5763-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="a5763-163">Bayes-Dirichlet-Äquivalent mit uniformer A-priori-Verteilung</span><span class="sxs-lookup"><span data-stu-id="a5763-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="a5763-164">Bei der Bayes-Dirichlet-Äquivalent-Bewertung wird auch die Bayes-Analyse zur Bewertung eines Netzwerks anhand eines gegebenen Datasets verwendet.</span><span class="sxs-lookup"><span data-stu-id="a5763-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="a5763-165">Diese Bewertungsmethode wurde von Heckerman entwickelt und basiert auf der von Cooper und Herskovits entwickelten BD-Metrik.</span><span class="sxs-lookup"><span data-stu-id="a5763-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="a5763-166">Bei der Dirichlet-Verteilung handelt es sich um eine Multinominalverteilung, die die bedingte Wahrscheinlichkeit jeder Netzwerkvariablen beschreibt und über viele für das Lernen nützliche Eigenschaften verfügt.</span><span class="sxs-lookup"><span data-stu-id="a5763-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="a5763-167">Die Methode Bayes-Dirichlet-Äquivalent mit uniformer A-priori-Verteilung setzt einen Sonderfall der Dirichlet-Verteilung voraus, in dem eine mathematische Konstante zur Erstellung einer festen oder einheitlichen Verteilung von A-priori-Zuständen verwendet wird.</span><span class="sxs-lookup"><span data-stu-id="a5763-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="a5763-168">Die Bayes-Dirichlet-Äquivalent-Bewertung unterstellt außerdem Likelihood-Äquivalenz, d. h. es wird nicht erwartet, dass die Daten äquivalente Strukturen unterscheiden können.</span><span class="sxs-lookup"><span data-stu-id="a5763-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="a5763-169">Anders ausgedrückt bedeutet dies, wenn der Score von „Wenn A dann B“ dem Score von „Wenn B dann A“ entspricht, dann lassen sich die Strukturen nicht anhand der Daten unterscheiden, und die Kausalität kann nicht aus den Daten gefolgert werden.</span><span class="sxs-lookup"><span data-stu-id="a5763-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="a5763-170">Weitere Informationen zu Bayes-Netzwerken und der Implementierung dieser Bewertungsmethoden finden Sie in [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span><span class="sxs-lookup"><span data-stu-id="a5763-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="a5763-171">Von Analysis Services-Algorithmen verwendete Funktionsauswahlmethoden</span><span class="sxs-lookup"><span data-stu-id="a5763-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="a5763-172">Die folgende Tabelle enthält die Algorithmen, welche die Funktionsauswahl unterstützen, die von einem Algorithmus verwendeten Funktionsauswahlmethoden und die Parameter, mit denen sich das Funktionsauswahlverhalten steuern lässt:</span><span class="sxs-lookup"><span data-stu-id="a5763-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="a5763-173">Algorithmus</span><span class="sxs-lookup"><span data-stu-id="a5763-173">Algorithm</span></span>|<span data-ttu-id="a5763-174">Analysemethode</span><span class="sxs-lookup"><span data-stu-id="a5763-174">Method of analysis</span></span>|<span data-ttu-id="a5763-175">Kommentare</span><span class="sxs-lookup"><span data-stu-id="a5763-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="a5763-176">Naive Bayes</span><span class="sxs-lookup"><span data-stu-id="a5763-176">Naive Bayes</span></span>|<span data-ttu-id="a5763-177">Shannon-Entropie</span><span class="sxs-lookup"><span data-stu-id="a5763-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="a5763-178">Bayes-Methode mit K2-A-priori-Verteilung</span><span class="sxs-lookup"><span data-stu-id="a5763-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="a5763-179">Bayes-Dirichlet mit uniformer A-priori-Verteilung (Standard)</span><span class="sxs-lookup"><span data-stu-id="a5763-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="a5763-180">Der Microsoft Naive Bayes-Algorithmus akzeptiert nur diskrete oder diskretisierte Attribute, daher kann er den Interessantheitsgrad nicht verwenden.</span><span class="sxs-lookup"><span data-stu-id="a5763-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="a5763-181">Weitere Informationen zu diesem Algorithmus finden Sie unter [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="a5763-182">Entscheidungsstrukturen</span><span class="sxs-lookup"><span data-stu-id="a5763-182">Decision trees</span></span>|<span data-ttu-id="a5763-183">Interessantheitsgrad</span><span class="sxs-lookup"><span data-stu-id="a5763-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="a5763-184">Shannon-Entropie</span><span class="sxs-lookup"><span data-stu-id="a5763-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="a5763-185">Bayes-Methode mit K2-A-priori-Verteilung</span><span class="sxs-lookup"><span data-stu-id="a5763-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="a5763-186">Bayes-Dirichlet mit uniformer A-priori-Verteilung (Standard)</span><span class="sxs-lookup"><span data-stu-id="a5763-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="a5763-187">Wenn irgendeine Spalte nicht binäre kontinuierliche Werte enthält, wird der Interessantheitsgrad für alle Spalten verwendet, um die Konsistenz zu gewährleisten.</span><span class="sxs-lookup"><span data-stu-id="a5763-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="a5763-188">Andernfalls wird die StandardFunktionsauswahlmethode oder die Methode angewendet, die Sie angegeben haben, als Sie das Modell erstellt haben.</span><span class="sxs-lookup"><span data-stu-id="a5763-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="a5763-189">Weitere Informationen zu diesem Algorithmus finden Sie unter [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="a5763-190">Neuronales Netzwerk</span><span class="sxs-lookup"><span data-stu-id="a5763-190">Neural network</span></span>|<span data-ttu-id="a5763-191">Interessantheitsgrad</span><span class="sxs-lookup"><span data-stu-id="a5763-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="a5763-192">Shannon-Entropie</span><span class="sxs-lookup"><span data-stu-id="a5763-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="a5763-193">Bayes-Methode mit K2-A-priori-Verteilung</span><span class="sxs-lookup"><span data-stu-id="a5763-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="a5763-194">Bayes-Dirichlet mit uniformer A-priori-Verteilung (Standard)</span><span class="sxs-lookup"><span data-stu-id="a5763-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="a5763-195">Der Microsoft Neural Networks-Algorithmus kann sowohl die Bayes- als auch die Entropie-basierte Methode verwenden, sofern die Daten kontinuierliche Spalten enthalten.</span><span class="sxs-lookup"><span data-stu-id="a5763-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="a5763-196">Weitere Informationen zu diesem Algorithmus finden Sie unter [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="a5763-197">Logistische Regression</span><span class="sxs-lookup"><span data-stu-id="a5763-197">Logistic regression</span></span>|<span data-ttu-id="a5763-198">Interessantheitsgrad</span><span class="sxs-lookup"><span data-stu-id="a5763-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="a5763-199">Shannon-Entropie</span><span class="sxs-lookup"><span data-stu-id="a5763-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="a5763-200">Bayes-Methode mit K2-A-priori-Verteilung</span><span class="sxs-lookup"><span data-stu-id="a5763-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="a5763-201">Bayes-Dirichlet mit uniformer A-priori-Verteilung (Standard)</span><span class="sxs-lookup"><span data-stu-id="a5763-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="a5763-202">Obwohl der Microsoft Logistic Regression-Algorithmus auf dem Microsoft Neural Network-Algorithmus basiert, können Sie keine logistischen Regressionsmodelle anpassen, um das Funktionsauswahlverhalten zu steuern. Deshalb wird die Funktionsauswahl immer standardmäßig nach der Methode ausgeführt, die für das Attribut am besten geeignet ist.</span><span class="sxs-lookup"><span data-stu-id="a5763-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="a5763-203">Wenn alle Attribute diskret oder diskretisiert sind, wird als Standardmethode Bayes-Dirichlet mit uniformer A-priori-Verteilung eingesetzt.</span><span class="sxs-lookup"><span data-stu-id="a5763-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="a5763-204">Weitere Informationen zu diesem Algorithmus finden Sie unter [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="a5763-205">Clustering</span><span class="sxs-lookup"><span data-stu-id="a5763-205">Clustering</span></span>|<span data-ttu-id="a5763-206">Interessantheitsgrad</span><span class="sxs-lookup"><span data-stu-id="a5763-206">Interestingness score</span></span>|<span data-ttu-id="a5763-207">Der Microsoft Clustering-Algorithmus kann diskrete oder diskretisierte Daten verwenden.</span><span class="sxs-lookup"><span data-stu-id="a5763-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="a5763-208">Da das Ergebnis jedes Attributs jedoch als Entfernung berechnet wird und als kontinuierliche Zahl dargestellt wird, muss der Interessantheitsgrad verwendet werden.</span><span class="sxs-lookup"><span data-stu-id="a5763-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="a5763-209">Weitere Informationen zu diesem Algorithmus finden Sie unter [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="a5763-210">Lineare Regression</span><span class="sxs-lookup"><span data-stu-id="a5763-210">Linear regression</span></span>|<span data-ttu-id="a5763-211">Interessantheitsgrad</span><span class="sxs-lookup"><span data-stu-id="a5763-211">Interestingness score</span></span>|<span data-ttu-id="a5763-212">Der Microsoft Linear Regression-Algorithmus kann nur den Interessantheitsgrad verwenden, da dieser nur kontinuierliche Spalten unterstützt.</span><span class="sxs-lookup"><span data-stu-id="a5763-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="a5763-213">Weitere Informationen zu diesem Algorithmus finden Sie unter [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="a5763-214">Zuordnungsregeln</span><span class="sxs-lookup"><span data-stu-id="a5763-214">Association rules</span></span><br /><br /> <span data-ttu-id="a5763-215">Sequenzclustering</span><span class="sxs-lookup"><span data-stu-id="a5763-215">Sequence clustering</span></span>|<span data-ttu-id="a5763-216">Nicht verwendet</span><span class="sxs-lookup"><span data-stu-id="a5763-216">Not used</span></span>|<span data-ttu-id="a5763-217">Die Funktionsauswahl wird nicht mit diesen Algorithmen aufgerufen.</span><span class="sxs-lookup"><span data-stu-id="a5763-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="a5763-218">Durch Festlegen der Parameter MINIMUM_SUPPORT und MINIMUM_PROBABILIITY lässt sich jedoch das Verhalten des Algorithmus steuern und die Größe der Eingabedaten falls notwendig reduzieren.</span><span class="sxs-lookup"><span data-stu-id="a5763-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="a5763-219">Weitere Informationen finden Sie unter [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) und [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="a5763-220">Zeitreihe</span><span class="sxs-lookup"><span data-stu-id="a5763-220">Time series</span></span>|<span data-ttu-id="a5763-221">Nicht verwendet</span><span class="sxs-lookup"><span data-stu-id="a5763-221">Not used</span></span>|<span data-ttu-id="a5763-222">Die Funktionsauswahl gilt nicht für Zeitreihenmodelle.</span><span class="sxs-lookup"><span data-stu-id="a5763-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="a5763-223">Weitere Informationen zu diesem Algorithmus finden Sie unter [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="a5763-224">Parameter für die Funktionsauswahl</span><span class="sxs-lookup"><span data-stu-id="a5763-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="a5763-225">In Algorithmen, die die Funktionsauswahl unterstützen, können Sie mithilfe der folgenden Parameter steuern, wann die Funktionsauswahl aktiviert wird.</span><span class="sxs-lookup"><span data-stu-id="a5763-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="a5763-226">Jeder Algorithmus verfügt über einen Standardwert für die Anzahl zulässiger Eingaben, Sie können diesen Standardwert jedoch überschreiben und die Anzahl der Attribute angeben.</span><span class="sxs-lookup"><span data-stu-id="a5763-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="a5763-227">In diesem Abschnitt sind die Parameter zur Verwaltung der Funktionsauswahl aufgeführt.</span><span class="sxs-lookup"><span data-stu-id="a5763-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="a5763-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="a5763-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="a5763-229">Falls ein Modell mehr Spalten enthält als durch die im *MAXIMUM_INPUT_ATTRIBUTES* -Parameter angegebene Zahl, ignoriert der Algorithmus alle Spalten, die er als irrelevant errechnet.</span><span class="sxs-lookup"><span data-stu-id="a5763-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="a5763-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="a5763-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="a5763-231">Entsprechend gilt: Falls ein Modell mehr vorhersagbare Spalten enthält als durch die im *MAXIMUM_OUTPUT_ATTRIBUTES* -Parameter angegebene Zahl, ignoriert der Algorithmus gleichermaßen alle Spalten, die er als irrelevant errechnet.</span><span class="sxs-lookup"><span data-stu-id="a5763-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="a5763-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="a5763-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="a5763-233">Wenn ein Modell mehr Fälle enthält, als im *MAXIMUM_STATES* -Parameter angegeben sind, werden die am wenigsten verbreiteten Status in einer Gruppe zusammengefasst und als fehlend behandelt.</span><span class="sxs-lookup"><span data-stu-id="a5763-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="a5763-234">Wird einer dieser Parameter auf 0 festgelegt, ist die Funktionsauswahl ausgeschaltet. Dies wirkt sich auf die Verarbeitungszeit und die Leistung aus.</span><span class="sxs-lookup"><span data-stu-id="a5763-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="a5763-235">Neben diesen Methoden für die Funktionsauswahl können Sie den Algorithmus bei der Identifizierung oder Heraufstufung aussagekräftiger Attribute unterstützen, indem Sie *Modellierungsflags* für das Modell oder *Verteilungsflags* für die Struktur festlegen.</span><span class="sxs-lookup"><span data-stu-id="a5763-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="a5763-236">Weitere Informationen zu diesen Konzepten finden Sie unter [Modellierungsflags &#40;Data Mining&#41;](modeling-flags-data-mining.md) und [Spaltenverteilungen &#40;Data Mining&#41;](column-distributions-data-mining.md).</span><span class="sxs-lookup"><span data-stu-id="a5763-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="a5763-237">Weitere Informationen</span><span class="sxs-lookup"><span data-stu-id="a5763-237">See Also</span></span>  
 [<span data-ttu-id="a5763-238">Anpassen von Miningmodellen und -strukturen</span><span class="sxs-lookup"><span data-stu-id="a5763-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
